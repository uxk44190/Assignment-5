# Assignment-5



Overview

Scaled dot-product attention computes how much each token should attend to others in a sequence.
Given Query (Q), Key (K), and Value (V) matrices, the attention mechanism:

Attention
(
ğ‘„
,
ğ¾
,
ğ‘‰
)
=
softmax
(
ğ‘„
ğ¾
ğ‘‡
ğ‘‘
ğ‘˜
)
ğ‘‰
Attention(Q,K,V)=softmax(
d
k
	â€‹

	â€‹

QK
T
	â€‹

)V
Steps Implemented

Compute similarity scores: 
ğ‘„
ğ¾
ğ‘‡
QK
T

Scale by 
ğ‘‘
ğ‘˜
d
k
	â€‹

	â€‹


Apply softmax normalization

Use attention weights to compute the context vector

Return both attention weights and context

Files Included

attention_numpy.py â€” implementation of scaled dot-product attention

Includes sample test with random Q, K, V inputs

Expected Output

Attention weights shape: (batch, seq_len, seq_len)

Context vector shape: (batch, seq_len, d_v)
